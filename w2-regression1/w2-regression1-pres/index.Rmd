---
title       : Регрессионный анализ, часть 1
subtitle    : Математические методы в зоологии - на R, осень 2013
author      : Марина Варфоломеева
job         : Каф. Зоологии беспозвоночных, СПбГУ
framework   : io2012        # {io2012, html5slides, shower, dzslides, ...}
highlighter : highlight.js  # {highlight.js, prettify, highlight}
hitheme     : idea      # 
widgets     : [mathjax]            # {mathjax, quiz, bootstrap}
mode        : standalone # {selfcontained, standalone, draft}
---

Знакомимся с линейными моделями
========================================================

- Модель простой линейной регрессии
- Проверка валидности модели (t- и F-критерии)
- Условия применимости
- Мощность линейной регрессии

```{r setup, include = FALSE, cache = FALSE}
#----------------------------------------------------------------
# RUN THE FRAGMENT BETWEEN LINES BEFORE COMPILING MARKDOWN
# to configure markdown parsing
options(markdown.extensions = 
          c("no_intra_emphasis",# skip markdown embedded in words
            "tables",           # create HTML tables
            "fenced_code",      # treat text as verbatim when surrounded with begin and ending lines with three ~ or ' characters.
            "autolink",         # create HTML links from urls and email addresses.
            "strikethrough",    # create strikethroughs by surrounding text with ~~.
            "lax_spacing",      # allow HTML tags inside paragraphs without being surrounded by newlines.
            "space_headers",    # add a space between header hashes and the header itself.
            "latex_math"))      # transforms all math equations into syntactically correct MathJax equations.
#--------------------------------------------------------------
# output options
options(width = 80, # set the maximum number of columns on a line
        scipen = 6, # fixed notation of floating point numbers, unless it is more than scipen digits wider, else - exponential notation
        digits = 3) # the number of digits to print when printing numeric values

# to render cyrillics in plots use cairo pdf
options(device = function(file, width = 7, height = 7, ...) {
  cairo_pdf(tempfile(), width = width, height = height, ...)
  })

# chunk default options
opts_chunk$set(fig.align='center',  # default figure alignment
               tidy = FALSE,        # save manual code formatting
               fig.width = 10,      # default figure width
               fig.height = 6)      # default figure height

# this allows for code formatting inline
knit_hooks$set(inline = function(x) {
   if (is.numeric(x)) return(knitr:::format_sci(x, 'latex'))
   x = as.character(x)
   h = knitr:::hilight_source(x, 'latex', list(prompt = FALSE, 
                                               size='normalsize', 
                                               highlight = FALSE))
   h = gsub("([_#$%&])", "\\\\\\1", h)
   h = gsub('(["\'])', '\\1{}', h)
   gsub('^\\\\begin\\{alltt\\}\\s*|\\\\end\\{alltt\\}\\s*$', '', h)})
```

```{r themes, echo = FALSE, cache = FALSE}
library(ggplot2)
library(grid)
theme_grey_slide <- function (base_size = 20, base_family = "") 
{
  # theme larger legend
  require(ggplot2)
  require(grid)
  theme_grey(base_size = base_size, base_family = base_family) %+replace% 
    theme(legend.key.size = unit(2, "lines"), 
          legend.text = element_text(lineheight=.8, hjust = 0))
}

theme_bw_slide <- function (base_size = 20, base_family = "") 
{
  # theme with larger legend
  require(ggplot2)
  require(grid)
  theme_bw(base_size = base_size, base_family = base_family) %+replace% 
    theme(legend.key.size = unit(2, "lines"),
          legend.text = element_text(lineheight=.8, hjust = 0.5))
}

theme_classic_slide <- function (base_size = 20, base_family = "") 
{
  # theme with larger legend
  require(ggplot2)
  require(grid)
  theme_classic(base_size = base_size, base_family = base_family) %+replace% 
    theme(legend.key.size = unit(2, "lines"), 
          legend.text = element_text(lineheight=.8, hjust = 0.5))
}

theme_set(theme_grey_slide())
```

--- .learning

Вы сможете
========================================================

- подобрать модель линейной регрессии
- проверить валидность модели при помощи t- или F-теста
- проверить выполнение условий применимости линейной регрессии
- расчитать мощность линейной регрессии

--- .segue

Модель простой линейной регрессии
========================================================

---

Линейная регрессия
========================================================

- простая

$$Y _i = β _0 + βx _i + \epsilon _i$$

- множественная

$$Y _i = β _0 + βx _{1 i} + + βx _{2 i} + ... + \epsilon _i$$

---

Запись моделей в R
========================================================

```{r eval=FALSE}
зависимая_переменная ~ модель
```

<br \>
$\hat y _i = b _0 + bx _i$ (простая линейная регрессия со свободным членом (intercept))
  - Y ~ X
  - Y ~ 1 + X 
  - Y ~ X + 1

$\hat y _i = bx _i$ (простая линейная регрессия без свободного члена)
  - Y ~ X - 1
  - Y ~ -1 + X

$\hat y _i = b _0$ (уменьшенная модель, линейная регрессия y от свободного члена)
  - Y ~ 1
  - Y ~ 1 - X

--- .prompt

Запишите в нотации R
========================================================
эти модели линейных регрессий

- $\hat y _i = b _0 + bx _{1 i} + bx _{2 i} + bx _{3 i}$ (множественная линейная регрессия со свободным членом)

- $\hat y _i = b _0 + bx _{1 i} + bx _{3 i}$ (уменьшенная модель множественной линейной регрессии, без $X2$)

*** pnotes

$\hat y _i = b _0 + bx _{1 i} + bx _{2 i} + bx _{3 i}$ 
  - Y ~ X1 + X2 + X3
  - Y ~ 1 + X1 + X2 + X3

$\hat y _i = b _0 + bx _{1 i} + bx _{3 i}$ 
  - Y ~ X1 + X3
  - Y ~ 1 + X1 + X3

--- &twocol

Минимизируем остаточную изменчивость
========================================================

*** left

$Y _i = β _0 + βx _i + \epsilon _i$ - модель регрессии

$\hat y _i = b _0 + b _1 x _i$ - оценка модели

нужно оценить $\beta _0$, $\beta _1$ и $σ^2$

![Линия регрессии по методу наименьших квадратов](./assets/img/OLS-regression-line.png "Линия регрессии по методу наименьших квадратов")

*** right

- Метод наименьших квадратов (Ordinary Least Squares, см. рис.)

<br />
<br />
Еще есть методы максимального правдоподобия (Maximum Likelihood, REstricted Maximum Likelihood)

<div class="footnote">Рисунок из кн. Quinn, Keough, 2002, стр. 85, рис. 5.6 a</div>

---

Оценки параметров линейной регрессии
========================================================

минимизируют $\sum{(y _i - \hat y _i)^2}$, т.е. остатки.

Параметры     | Оценки параметров | Стандартные ошибки оценок
------- | --------- | -----
$\beta _1$    | $$b _1 = \frac {\sum _{i=1}^{n} {[(x _i - \bar x)(y _i - \bar y)]}}{\sum _{i=1}^{n} {(x _i - \bar x)^2}}$$      | $$SE _{b _1} = \sqrt{\frac{MS _e}{\sum _{i=1}^{n} {(x _i - \bar x)^2}}}$$
$\beta _0$    | $$b _0 = \bar y - b _1 \bar x$$  | $$SE _{b _0} = \sqrt{MS _e [\frac{1}{n} + \frac{\bar x}{\sum _{i=1}^{n} {(x _i - \bar x)^2}}]}$$
$\epsilon _i$ | $$e _i = y _i - \hat y _i$$      | $$\approx \sqrt{MS _e}$$

>-  Стандартные ошибки коэффициентов нужны
  - для построения доверительных интервалов
  - для статистических тестов

<div class="footnote">Таблица из кн. Quinn, Keough, 2002, стр. 86, табл. 5.2</div>

---

Коэффициенты регрессии
========================================================

![Интерпретация коэффициентов регрессии](./assets/img/interpretation-of-regression-coefficients.png "Интерпретация коэффициентов регрессии")

<div class="footnote">Рисунок из кн. Logan, 2010, стр. 170, рис. 8.2</div>


>- Если нужно сравнивать - лучше стандартизованные (= "бета коэффициенты") коэффициенты (на след.лекции про сравнение моделей)
  - $b^\ast _1 = {b _1  {\sigma _x} \over {\sigma _y}}$
  - не зависят от масштаба

--- .sub-section &twocol

Пример: усыхающие личинки мучных хрущаков
=========================================================

Как зависит потеря влаги личинками [малого мучного хрущака](http://ru.wikipedia.org/wiki/Хрущак_малый_мучной) _Tribolium confusum_ от влажности воздуха? (Nelson, 1964)

*** left

9 экспериментов, продолжительность 6 дней
- разная относительная влажность воздуха, % (`humidity`)
- измерена потеря влаги, мг (`weightloss`)

Данные в файлах `nelson.xlsx` и `nelson.csv`

*** right

![Малый мучной хрущак Tribolium confusum](./assets/img/Tribolium_confusum.jpg "Малый мучной хрущак Tribolium confusum")

<div class = "footnote">Данные из Sokal, Rohlf, 1997, табл. 14.1 по Logan, 2010. глава 8, пример 8c</div>

---

Читаем данные из файла и знакомимся с ними
=========================================================

```{r include = FALSE}
setwd("/media/data/ProjectsWork/lect-mathmethr-2013/w2-regression1")
library(XLConnect)
wb <- loadWorkbook("./data/nelson.xlsx")
nelson <- readWorksheet(wb, sheet = 1)
```

Внимание, установите рабочую директорию, или используйте полный путь к файлу

```{r eval = FALSE}
setwd("C:\\mathmethr\week2")
## из .xlsx
library(XLConnect)
wb <- loadWorkbook(".\data\nelson.xlsx")
nelson <- readWorksheet(wb, sheet = 1)
## или из .csv 
# nelson <- read.table(file=".\data\nelson.xlsx", header = TRUE, sep = "\t", dec = ".") 
```
```{r}
str(nelson)
head(nelson)
```

---
Как зависит потеря веса от влажности? График рассеяния.
========================================================
```{r}
library(ggplot2)
p_nelson <- ggplot(data=nelson, aes(x = humidity, y = weightloss)) + 
  geom_point() + 
  labs(x = "Относительная влажность, %", y = "Потеря веса, мг")
p_nelson
```

---
Внешний вид графиков можно менять при помощи тем
========================================================
```{r include = FALSE}
p1 <- p_nelson + theme_classic_slide(18)
p2 <- p_nelson + theme_bw_slide(18)
```

```{r eval = FALSE}
p_nelson + theme_classic()
p_nelson + theme_bw()
theme_set(theme_classic()) # устанавливаем понравившуюся тему до конца сессии
```

```{r echo=FALSE, fig.width=13}
library(gridExtra)
new_plot <- grid.arrange (p1, p2, ncol = 2)
theme_set(theme_classic_slide(18))
```

---
Подбираем параметры линейной модели
========================================================
```{r}
nelson_lm <- lm(weightloss ~ humidity, nelson)
summary(nelson_lm)
```

---

Добавим линию регрессии на график
========================================================
```{r}
p_nelson + geom_smooth(method = "lm")
```

--- .prompt

Как вы думаете,
========================================================
что это за серая область вокруг линии регрессии?

```{r}
p_nelson + geom_smooth(method = "lm")
```

*** pnotes

Это 95% доверительная зона регрессии.

В ней с 95% вероятностью лежит регрессионная прямая.   

Возникает из-за неопределенности оценок коэффициентов регрессии.


--- .segue

Неопределенность оценок коэффициентов и предсказанных значений
========================================================

---
Неопределенность оценок коэффициентов
========================================================

>- __Доверительный интервал коэффициента__
  - зона, в которой с $(1 - \alpha) \cdot 100\%$ вероятностью содержится среднее значение коэффициента<br />
  - <large>$b _1 \pm t _{\alpha, df = n - 2}SE _{b _1}$</large><br />
  - $\alpha = 0.05$ => $(1 - 0.05) \cdot 100\% = 95\%$ интервал
<br /><br /><br />
>- __Доверительная зона регрессии__
  - зона, в которой с $(1 - \alpha) \cdot 100$% вероятностью лежит регрессионная прямая

---
Находим доверительные интервалы коэффициентов
===================================================
```{r}
# Вспомните, в выдаче summary(nelson_lm) были только оценки коэффициентов 
# и стандартные ошибки

# оценки коэффициентов отдельно
coef(nelson_lm)

# доверительные интервалы коэффициентов
confint(nelson_lm)
```


--- &twocol w1:60% w2:40% 

Оценим, какова средняя потеря веса при заданной влажности 
===================================================
__Нельзя давать оценки вне интервала значений $X$!__

*** left

```{r fig.height = 7}
# новые данные для предсказания значений
newdata <- data.frame(humidity = c(50, 100)) 
predict(nelson_lm, newdata, 
        interval = "confidence", se = TRUE) 
# доверительный интервал к среднему значению
```

*** right

```{r echo = FALSE, fig.width = 5, fig.height = 4.5}
pr1 <- predict(nelson_lm, newdata, 
        interval = "confidence", se = TRUE) 
p_nelson + geom_smooth(method = "lm", se = FALSE)
```

>- При 50 и 100% относительной влажности ожидаемая средняя потеря веса жуков будет `r round(pr1$fit[1,1], 1)` $\pm$ `r round(pr1$fit[1,1] - pr1$fit[1,2], 1)` и `r round(pr1$fit[2,1], 1)` $\pm$ `r round(pr1$fit[2,1] - pr1$fit[2,2], 1)`, соответственно.

---

Строим доверительную зону регрессии
===================================================
```{r eval = FALSE}
p_nelson + geom_smooth(method = "lm") + 
  ggtitle ("95% доверительная зона регрессии")
p_nelson + geom_smooth(method = "lm", level = 0.99) + 
  ggtitle ("99% доверительная зона регрессии")
```
```{r echo=FALSE, fig.width=13}
p1 <- p_nelson + geom_smooth(method = "lm") + ggtitle ("95% доверительная зона регрессии")
p2 <- p_nelson + geom_smooth(method = "lm", level = 0.99) + ggtitle ("99% доверительная зона регрессии")
grid.arrange(p1, p2, ncol = 2)
```

---

Неопределенность оценок предсказанных значений
===================================================

>- __Доверительный интервал к предсказанному значению__
  - зона в которую попадают $(1 - \alpha) \cdot 100$% значений $\hat y _i$ при данном $x _i$<br />
  - <large>$\hat y _i \pm t _{0.05, n - 2}SE _{\hat y _i}$</large><br />
  - <large>$SE _{\hat y} = \sqrt{MS _{e} [1 + \frac{1}{n} + \frac{(x _{prediction} - \bar x)^2} {\sum _{i=1}^{n} {(x _{i} - \bar x)^2}}]}$</large>
<br /><br /><br />

>- __Доверительная область значений регрессии__
  - зона, в которую попадает $(1 - \alpha) \cdot 100$% всех предсказанных значений

--- &twocol w1:60% w2:40% 

Предсказываем для новых значений
===================================================
__Нельзя использовать для предсказаний вне интервала значений $X$!__

*** left

```{r fig.height = 7}
# новые данные для предсказания значений
newdata <- data.frame(humidity = c(50, 100)) 
predict(nelson_lm, newdata, 
        interval = "prediction", se = TRUE)
# зона, в которой будут лежать 95% всех значений
```

*** right

```{r echo = FALSE, fig.width = 5, fig.height = 4.5}
p_nelson + geom_smooth(method = "lm", se = FALSE)
pr2 <- predict(nelson_lm, newdata, 
        interval = "prediction", se = TRUE)
```

>- У 95% жуков при 50 и 100% относительной влажности будет потеря веса будет в пределах `r round(pr2$fit[1,1], 1)` $\pm$ `r round(pr2$fit[1,1] - pr2$fit[1,2], 1)` и `r round(pr2$fit[2,1], 1)` $\pm$ `r round(pr2$fit[2,1] - pr2$fit[2,2], 1)`, соответственно.

---
Данные для доверительной области значений
===================================================
```{r}
# предсказанные значения для исходных данных
predict(nelson_lm, interval = "prediction")
# объединим с исходными данными в новом датафрейме - для графиков
nelson_with_pred <- data.frame(nelson, predict(nelson_lm, interval = "prediction"))
```

---
Строим доверительную область значений и доверительный интервал
===================================================
```{r warning=FALSE, fig.height = 5, fig.width = 13}
p_nelson + geom_smooth(method = "lm", aes(fill = "Доверительный интервал"), alpha = 0.4) +
  geom_ribbon(data = nelson_with_pred, 
              aes(y = fit, ymin = lwr, ymax = upr, fill = "Доверительная область значений"), 
              alpha = 0.2) +
  scale_fill_manual('Интервалы', values = c('green', 'blue'))
```

--- .segue

Проверка валидности модели
========================================================
## $H _0: \beta _1 = 0$

## или t-, или F-тест

---

Проверка при помощи t-критерия
========================================================

$H _0 : b _1 = \theta$, $\theta = 0$
$$t = \frac{b _1 - \theta}{SE _{b _1}}$$
$df = n - 2$

---

Проверка коэффициентов с помощью t-критерия есть в сводке модели
========================================================
```{r}
summary(nelson_lm)
```

>- Увеличение относительной влажности привело к достоверному замедлению потери веса жуками ($b _1 = -0.053$, $t = - 16.35$, $p < 0.01$)

---

Проверка при помощи F-критерия
========================================================

$H _0: \beta _1 = 0$ 

- Та же самая нулевая гипотеза. Как так получается?

---

Общая изменчивость - отклонения от общего среднего значения
========================================================
$SS _{total}$
--------------------------------------------------------

<div style="line-height: 500px;">
<img style="vertical-align: middle;" src="./assets/img/total-variation.png" alt="Общая изменчивость" title="Общая изменчивость">
</div>

<div class="footnote">
Рисунок из кн. Logan, 2010, стр. 172, рис. 8.3 a
</div>

--- &twocol

$SS _{total} = SS _{regression} + SS _{error}$
--------------------------------------------------------

*** left

<div style="line-height: 500px;">
<img style="vertical-align: middle;" src="./assets/img/total-variation.png" alt="Общая изменчивость" title="Общая изменчивость">
</div>

<div class="footnote">
Рисунок из кн. Logan, 2010, стр. 172, рис. 8.3 a-c
</div>

*** right

![Объясненная регрессией изменчивость](./assets/img/explained-variation.png "Объясненная регрессией изменчивость")
![Остаточная изменчивость](./assets/img/residual-variation.png "Остаточная изменчивость")

--- &twocol

Если зависимости нет, $b _1 = 0$
========================================================

*** left
<div>
Тогда $\hat y _i = \bar y _i$
<br />
и $MS _{regression} \approx MS _{error}$
</div>

<div style="line-height: 400px;">
<img style="vertical-align: middle;" src="./assets/img/total-variation.png" alt="Общая изменчивость" title="Общая изменчивость">
</div>

<div class="footnote">
Рисунок из кн. Logan, 2010, стр. 172, рис. 8.3 a-c
</div>

*** right

![Объясненная регрессией изменчивость](./assets/img/explained-variation.png "Объясненная регрессией изменчивость")
![Остаточная изменчивость](./assets/img/residual-variation.png "Остаточная изменчивость")

*** pnotes

Что оценивают средние квадраты отклонений?

Источник изменчивости  |  Суммы квадратов отклонений<br />SS |   Число степеней свободы<br />df   | Средний квадрат отклонений<br />MS | Ожидаемый средний квадрат
---------------------- | ----- | ------ | ------------------- | -----
Регрессия | $$\sum{(\bar y - \hat y _i)^2}$$ | $$1$$ | $$\frac{\sum _{i=1}^{n}{(\bar y - \hat y _i)^2}}{1}$$ | $$\sigma _{\epsilon} ^2 + {\beta _1} ^2 \sum _{i=1}^{n} {(x _i - \bar x)^2}$$
Остаточная | $$\sum{(y _i - \hat y _i)^2}$$ | $$n - 2$$ | $$\frac{\sum _{i=1}^{n}{(y _i - \hat y _i)^2}}{n - 2}$$ | $$\sigma _{\epsilon} ^2$$
Общая | $$\sum {(\bar y - y _i)^2}$$ | $$n - 1$$ | 

<br />
Если $b _1 = 0$, тогда $\hat y _i = \bar y _i$ и $MS _{regression} \approx MS _{error}$

Тестируем:

$$F = {MS _{regression} \over MS _{error}}$$

--- &twocol

F-критерий и распределение F-статистики
========================================================

$$F = \frac {Объясненная\ изменчивость}{Необъясненная\ изменчивость} = \frac {MS _{regression}} {MS _{error}}$$


*** left

![Распределение F-статистики](./assets/img/f-distribution.png "Распределение F-статистики")

F-распределение при $H _0: b _1 = 0$ 

*** right

<br />
<br />
Зависит от

  - $\alpha$
  
  - $df _{regression}$
  
  - $df _{error}$

<div class="footnote">Рисунок с изменениями из кн. Logan, 2010, стр. 172, рис. 8.3 d</div>

---

Таблица результатов дисперсионного анализа
========================================================

Источник изменчивости  |  Суммы квадратов отклонений,<br /> SS   |   Число степеней свободы,<br /> df   | Средний квадрат отклонений,<br /> MS | F  
---------------------- | --------- | ------ | ------------------- | -----
Регрессия | $$SS _r = \sum{(\bar y - \hat y _i)^2}$$ | $$df _r = 1$$ | $$MS _r = \frac{SS _r}{df _r}$$ | $$F _{df _r, df _e} = \frac{MS _r}{MS _e}$$
Остаточная | $$SS _e = \sum{(y _i - \hat y _i)^2}$$ | $$df _e = n - 2$$ | $$MS _e = \frac{SS _e}{df _e}$$ | 
Общая | $$SS _t = \sum {(\bar y - y _i)^2}$$ | $$df _t = n - 1$$ | 

<br />
>- Минимальное упоминание в тексте - $F _{df _r, df _e}$, $p$

---

Проверяем валидность модели при помощи F-критерия
========================================================
```{r}
nelson_aov <- aov(nelson_lm)
summary(nelson_aov)
```

<br />
>- Количество влаги, потерянной жуками в период эксперимента, достоверно зависело от уровня относительной влажности ($F _{1, 7} = 267$, $p < 0.01$).

--- .segue

Оценка качества подгонки модели
========================================================

---

Коэффициент детерминации
========================================================
доля общей изменчивости, объясненная линейной связью x и y

$R^2 = \frac {SS _r} {SS _t}$

$0 \le R^2 \le 1$

<br /><br />
Иначе рассчитывается как $R^2 = r^2$

---

Коэффициент детерминации 
========================================================
можно найти в сводке модели

>- Осторожно, не сравнивайте $R^2$ моделей с разным числом параметров, для этого есть $R^2 _{adjusted}$

```{r}
summary(nelson_lm)
```


--- .segue

Условия применимости простой линейной регрессии и анализ остатков
========================================================

---

Условия применимости простой линейной регрессии 
========================================================
должны выполняться, чтобы тестировать гипотезы

1. Независимость

1. Линейность 

1. Нормальное распределение

1. Гомогенность дисперсий

---

Независимость
========================================================
>- Значения $y _i$ должны быть независимы друг от друга
  - берегитесь псевдоповторностей
  - берегитесь автокорреляций (например, временных)
- Контролируется на этапе планирования
- Проверяем на графике остатков

---

Линейность связи
========================================================

>- проверяем на графике рассеяния исходных данных
- проверяем на графике остатков

--- &twocol

Вот, что бывает, если неглядя применять линейную регрессию
========================================================

*** left

[Квартет Энскомба](http://ru.wikipedia.org/wiki/Квартет_Энскомба) - примеры данных, где регрессии одинаковы во всех случаях (Anscombe, 1973)

$y _i = 3.0 + 0.5 x _i$,

$r^2 = 0.68$, 

$H _0: \beta _1 = 0$, $t = 4.24$, $p = 0.002$

*** right

![Квартет Анскомба](./assets/img/anscombe.png "Квартет Анскомба")

<div class="footnote">Рисунок из кн. Quinn, Keough, 2002, стр. 97, рис. 5.9</div>

--- &twocol

Нормальное распределение
========================================================

*** left

Нужно, т.к. в модели $Y _i = β _0 + βx _i + \epsilon _i$

$Y \sim N(0,\sigma^2)$

<br />

>- К счастью, это значит, что $\epsilon _i \sim N(0,\sigma^2)$

<br /><br /><br />

>- Нужно для тестов параметров, а не для подбора методом наименьших квадратов

>- Тесты устойчивы к небольшим отклонениям от нормального распределения

>- Проверяем распределение остатков на нормально-вероятностном графике


*** right

![Условие нормальности: Нормальное распределение Y для каждого X](./assets/img/normality-assumption.png "Условие нормальности: Нормальное распределение Y для каждого X")

<div class="footnote">Рисунок из кн. Quinn, Keough, 2002, стр. 85, рис. 5.5</div>

--- &twocol

Гомогенность дисперсий
========================================================

*** left

Нужно, т.к. в модели $Y _i = β _0 + βx _i + \epsilon _i$

$Y \sim N(0,\sigma^2)$,

и дисперсии $\sigma^2 _1 = \sigma^2 _2 = ... = \sigma^2 _i$ для каждого $Y _i$ 

>- К счастью, поскольку $\epsilon _i \sim N(0,\sigma^2)$, можно проверить равенство дисперсий остатков $\epsilon _i$

<br />
>- Нужно и важно для тестов параметров

>- Проверяем на графике остатков по отношению к предсказанным значениям

>- Можно сделать тест С Кокрана (Cochran's C), но только если несколько значений y для каждого x

*** right

![Условие гомогенности дисперсий: Дисперсии Y равны для каждого X](./assets/img/normality-assumption.png "Условие гомогенности дисперсий: Дисперсии Y равны для каждого X")

<div class="footnote">Рисунок из кн. Quinn, Keough, 2002, стр. 85, рис. 5.5</div>

--- &twocol

Диагностика регрессии по графикам остатков
========================================================

*** left

![Диагностика регрессии по графикам остатков](./assets/img/assumption-violations-on-residual-plots.png "Диагностика регрессии по графикам остатков")

<div class="footnote">Рисунок из кн. Logan, 2010, стр. 174, рис. 8.5 d</div>

*** right

<br />
>- a - все в порядке
>- b - разброс остатков разный (wedge-shaped pattern)
>- c - разброс остатков одинаковый, но нужны дополнительные предикторы
>- d - к нелинейной зависимости применили линейную регрессию

--- &twocol

Какие наблюдения влияют на ход регрессии больше других?
========================================================

*** left

Влиятельные наблюдения, выбросы, outliers

- большая абсолютная величина остатка
- близость к краям области определения ([leverage](http://en.wikipedia.org/wiki/Leverage_(statistics\) ) - рычаг, "сила"; иногда называют hat)

<br />
>-
  - 1 - не влияет
  - 2 - умеренно влияет (большой остаток, малая сила влияния)
  - 3 - очень сильно влияет (большой остаток, большая сила влияния)

*** right

![Влиятельные наблюдения](./assets/img/influential-observations.png "Влиятельные наблюдения")

<div class="footnote">Рисунок из кн. Quinn, Keough, 2002, стр. 96, рис. 5.8</div>

--- &twocol

Как оценить влиятельность наблюдений
========================================================

*** left

[Расстояние Кука (Cook's d)](http://en.wikipedia.org/wiki/Cook's_distance) (Cook, 1977)

>- Учитывает одновременно величину остатка и близость к краям области определения (leverage)

>- Условное пороговое значение:  
выброс, если $d \ge 4/(N - k - 1)$,  
где $N$ - объем выборки,  
$k$ - число предикторов.  
<br />Дж. Фокс советует не обращать внимания на пороговые значения (Fox, 1991).

>- Что делать с влиятельными точками?
  - Проверить, не ошибка ли это.  
    Если это не ошибка, не удалять - обсуждать!
  - Проверить, что будет, если их исключить из модели

*** right

![Влиятельные наблюдения](./assets/img/influential-observations.png "Влиятельные наблюдения")

<div class="footnote">Рисунок из кн. Quinn, Keough, 2002, стр. 96, рис. 5.8</div>

--- .segue .prompt

Проверим условия применимости
========================================================

--- .prompt

Проверьте линейность связи, 
========================================================
постройте для этого график рассеяния

```{r eval = FALSE}
ggplot()
aes()
geom_point()
```


*** pnotes

```{r}
ggplot(data = nelson, aes(x = humidity, y = weightloss)) + 
  geom_point()
```

---

Для анализа остатков выделим нужные данные в новый датафрейм
========================================================

```{r}
# library(ggplot2)
nelson_diag <- fortify(nelson_lm)
names(nelson_diag) # названия переменных
```

<br />

>- Кроме `weightloss` и `humidity` нам понадобятся  
  `.cooksd` - расстояние Кука  
  `.fitted` - предсказанные значения  
  `.resid` - остатки  
  `.stdresid` - стандартизованные остатки

--- .prompt

Постройте график зависимости остатков от предиктора (относительная влажность),  
========================================================
используя данные для анализа остатков из `nelson_diag`

```{r eval = FALSE}
names() 
ggplot()
aes()
geom_point()
```

>- По абсолютным остаткам сложно сказать, большие они или маленькие. Нужна стандартизация

*** pnotes

```{r}
ggplot(data = nelson_diag, aes(x = humidity, y = .stdresid)) +
  geom_point()
```
По абсолютным остаткам сложно сказать, большие они или маленькие. Нужна стандартизация

--- .prompt

Постройте график зависимости __стандартизованных остатков__ от предсказанных значений
========================================================

Стандартизованные остатки $\frac {y _i - \hat y _i} {\sqrt{MS _e}}$
>- можно сравнивать между регрессиями
>- можно сказать, какие остатки большие, какие нет
  - $\le 2 SD$ - обычные
  - $> 3 SD$ - редкие


```{r eval = FALSE}
ggplot()
aes()
geom_point()
```


*** pnotes

```{r}
ggplot(data = nelson_diag, aes(x = .fitted, y = .stdresid)) +
  geom_point()
```

---

График станет информативнее, если кое-что добавить
========================================================

```{r}
ggplot(data = nelson_diag, aes(x = .fitted, y = .stdresid)) +
  geom_point(aes(size = .cooksd)) +          # расстояние Кука
  geom_smooth(method="loess", se = FALSE) +  # линия тренда
  geom_hline(yintercept = 0)                 # горизонтальная линия на уровне y = 0
```

--- .prompt &twocol

Какие выводы можно сделать по графику остатков?
========================================================

*** left

```{r resplot, fig.height=6, fig.width=5.5, echo=FALSE}
ggplot(data = nelson_diag, aes(x = .fitted, y = .stdresid)) +
  geom_point(aes(size = .cooksd)) +          # расстояние Кука
  geom_smooth(method="loess", se = FALSE) +  # линия тренда
  geom_hline(yintercept = 0)                 # горизонтальная линия на уровне y = 0
```

*** right

>- Стандартизованные остатки умеренной величины (в пределах двух стандартных отклонений), их разброс почти одинаков
>- Мало точек, чтобы надежно оценить наличие трендов среди остатков

---

Нормально-вероятностный график остатков
========================================================
Используется, чтобы оценить форму распределения.

Если точки лежат на одной прямой - нормальное распределение.

```{r fig.width = 6}
ggplot(nelson_diag, aes(sample = .stdresid)) + 
  geom_point(stat = "qq")
```


--- &twocol

На нормально-вероятностный график можно добавить линию
========================================================
```{r qqplot, eval = FALSE}
source(url("http://stat511.cwick.co.nz/code/stat_qqline.r"))
ggplot(nelson_diag, aes(sample = .stdresid)) + 
  geom_point(stat = "qq") + 
  stat_qqline()
```

*** left

```{r qqplot, warning = FALSE, message=FALSE, eval = TRUE, echo=FALSE, fig.width=6}
```

*** right

>- Небольшие отклонения от нормального распределения,но мало точек, чтобы оценить это с уверенностью
- Небольшими отклонениями можно пренебречь, т.к. тесты, используемые в регрессионном анализе к ним устойчивы.

--- .segue

Мощность линейной регрессии 
========================================================

---

Величина эффекта из общих соображений
========================================================
```{r}
library(pwr)
cohen.ES(test="f2",size="large")
```

*** pnotes

Слабый | Умеренный | Сильный
--- | --- | ---
0.02 | 0.15 | 0.35

---
Величина эффекта для набора предикторов по $R^2$
======================================================

$$f^2 = \frac {R^2}{1 - R^2}$$

$R^2$ - коэффициент детерминации

--- .prompt

Посчитайте 
======================================================
какой нужен объем выборки, чтобы с вероятностью 0.8 обнаружить умеренную зависимость при помощи простой линейной регрессии?

```{r eval=FALSE}
cohen.ES()
pwr.f2.test()
```

*** pnotes

```{r}
pwr.f2.test(u=1, v=NULL, f2=0.15, sig.level=0.05, power=0.8)
```

---

Take home messages
========================================================
>- Модель простой линейной регрессии $y _i = \beta _0 + \beta _1 \dot x _i + \epsilon _i$

>- В оценке коэффициентов регрессии и предсказанных значений существует неопределенность. Доверительные интервалы можно расчитать, зная стандартные ошибки.

>- Валидность модели линейной регрессии можно проверить при помощи t- или F-теста. $H _0: \beta _1 = 0$

>- Условия применимости линейной регрессии:
  - Независимость
  - Линейная связь
  - Нормальное распределение
  - Гомогенность дисперсий

>- Анализ остатков используется для диагностики условий применимости линейной регрессии
  
>- Мощность линейной регрессии можно рассчитать, например, как мощность F-критерия

---

Дополнительные ресурсы
========================================================
- Гланц, 1999, стр. 221-244
- Logan, 2010, pp. 170-207
- Quinn, Keough, 2002, pp. 78-110
- [Open Intro to Statistics](https://docs.google.com/viewer?docex=1&url=http://www.openintro.org/stat/down/OpenIntroStatSecond.pdf): [Chapter 7. Introduction to linear regression](https://docs.google.com/viewer?docex=1&url=http://www.openintro.org/stat/down/oiStat2_07.pdf), pp. 315-353.  
- Sokal, Rohlf, 1995, pp. 451-491
- Zar, 1999, pp. 328-355
