---
title       : Регрессионный анализ, часть 2
subtitle    : Математические методы в зоологии - на R, осень 2013
author      : Марина Варфоломеева
job         : Каф. Зоологии беспозвоночных, СПбГУ
framework   : io2012        # {io2012, html5slides, shower, dzslides, ...}
highlighter : highlight.js  # {highlight.js, prettify, highlight}
hitheme     : idea      # 
widgets     : [mathjax]            # {mathjax, quiz, bootstrap}
mode        : standalone # {selfcontained, standalone, draft}
---

Когда и какую регрессию можно применять
========================================================

- Условия применимости регрессионного анализа
- Мощность линейной регрессии







--- .learning

Вы сможете
========================================================
- Проверить условия применимости простой линейной регрессии
- Рассчитать мощность линейной регрессии

--- .sub-section &twocol

Пример: усыхающие личинки мучных хрущаков
=========================================================

Как зависит потеря влаги личинками [малого мучного хрущака](http://ru.wikipedia.org/wiki/Хрущак_малый_мучной) _Tribolium confusum_ от влажности воздуха? (Nelson, 1964)





*** left


```r
# Внимание, установите рабочую директорию, 
# или используйте полный путь к файлу
setwd("C:/mathmethr/week2")
## из .xlsx
library(XLConnect)
wb <- loadWorkbook("./data/nelson.xlsx")
nelson <- readWorksheet(wb, sheet = 1)
## или из .csv 
# nelson <- read.table(file="./data/nelson.csv",
#                      header = TRUE, sep = "\t",
#                      dec = ".") 
```


*** right

![Малый мучной хрущак Tribolium confusum](./assets/img/Tribolium_confusum.jpg "Малый мучной хрущак Tribolium confusum")

<div class = "footnote">Данные из Sokal, Rohlf, 1997, табл. 14.1 по Logan, 2010. глава 8, пример 8c</div>

--- 

Как зависит потеря веса от влажности? График рассеяния.
========================================================

```r
library(ggplot2)
theme_set(theme_classic()) # устанавливаем понравившуюся тему до конца сессии
p_nelson <- ggplot(data=nelson, aes(x = humidity, y = weightloss)) + 
  geom_point() + 
  geom_smooth(method = "lm", colour = "red") +
  labs(x = "Относительная влажность, %", y = "Потеря веса, мг")
p_nelson
```

<img src="figure/unnamed-chunk-4.png" title="plot of chunk unnamed-chunk-4" alt="plot of chunk unnamed-chunk-4" style="display: block; margin: auto;" />


---

Проверяем, есть ли зависимость потери веса от влажности с помощью линейной регрессии
========================================================


```r
# линейная регрессия из прошлой лекции
nelson_lm <- lm(weightloss ~ humidity, nelson)
summary(nelson_lm)
```

```
## 
## Call:
## lm(formula = weightloss ~ humidity, data = nelson)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -0.4640 -0.0344  0.0167  0.0746  0.4524 
## 
## Coefficients:
##             Estimate Std. Error t value      Pr(>|t|)    
## (Intercept)  8.70403    0.19156    45.4 0.00000000065 ***
## humidity    -0.05322    0.00326   -16.4 0.00000078161 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 0.297 on 7 degrees of freedom
## Multiple R-squared:  0.974,	Adjusted R-squared:  0.971 
## F-statistic:  267 on 1 and 7 DF,  p-value: 0.000000782
```


---

Зависимость потери веса от влажности можно описать уравнением
========================================================

Для этого подставим коэффициенты в уравнение линейной регрессии $y = b _0 + b _1 x$


```r
coef(nelson_lm) # Коэффициенты регрессии
```

```
## (Intercept)    humidity 
##      8.7040     -0.0532
```


$weightloss =$  8.7 -0.05  $humidity$

Чаще более академические обозначения:

$y =$ 8.7 -0.05 $x$, $R^2 =$ 0.974

Потеря веса мучными хрущаками в результате высыхания достоверно зависит от относительной влажности ($\beta _1 =$ -0.05 $\pm$ 0.01, $p < 0.01$)

--- .segue

Условия применимости простой линейной регрессии и анализ остатков
========================================================
Насколько можно доверять оценкам коэффициентов, которые мы получили?
--------------------------------------------------------

---

Условия применимости простой линейной регрессии 
========================================================
должны выполняться, чтобы тестировать гипотезы

1. Независимость

1. Линейность 

1. Нормальное распределение

1. Гомогенность дисперсий

--- 

1. Независимость
========================================================

- Значения $y _i$ должны быть независимы друг от друга
  - берегитесь псевдоповторностей
  - берегитесь автокорреляций (например, временных)
- Контролируется на этапе планирования
- Проверяем на графике остатков


![Остаточная изменчивость](./assets/img/assumption-12.png "Остаточная изменчивость")

<div class="footnote">Рисунок из кн. Diez et al., 2010, стр. 332, рис. 7.8</div>

---

2. Линейность связи
========================================================
- проверяем на графике рассеяния исходных данных
- проверяем на графике остатков

<br /><br /><br />

![Остаточная изменчивость](./assets/img/assumption-12.png "Остаточная изменчивость")

<div class="footnote">Рисунок из кн. Diez et al., 2010, стр. 332, рис. 7.8</div>

--- &twocol

Вот, что бывает, если неглядя применять линейную регрессию
========================================================

*** left

[Квартет Энскомба](http://ru.wikipedia.org/wiki/Квартет_Энскомба) - примеры данных, где регрессии одинаковы во всех случаях (Anscombe, 1973)

$y _i = 3.0 + 0.5 x _i$,

$r^2 = 0.68$, 

$H _0: \beta _1 = 0$, $t = 4.24$, $p = 0.002$

*** right

![Квартет Анскомба](./assets/img/anscombe.png "Квартет Анскомба")

<div class="footnote">Рисунок из кн. Quinn, Keough, 2002, стр. 97, рис. 5.9</div>

--- &twocol

3. Нормальное распределение
========================================================

*** left

Нужно, т.к. в модели $Y _i = β _0 + βx _i + \epsilon _i$

$Y \sim N(0,\sigma^2)$

<br />

>- К счастью, это значит, что $\epsilon _i \sim N(0,\sigma^2)$

<br /><br /><br />

>- Нужно для тестов параметров, а не для подбора методом наименьших квадратов

>- Тесты устойчивы к небольшим отклонениям от нормального распределения

>- Проверяем распределение остатков на нормально-вероятностном графике


*** right

![Условие нормальности и гомогенность дисперсий](./assets/img/normality-assumption.png "Условие нормальности и гомогенность дисперсий")

<div class="footnote">Рисунок из кн. Watkins et al., 2008, стр. 743, рис. 11.4</div>

--- &twocol

4. Гомогенность дисперсий
========================================================

*** left

Нужно, т.к. в модели $Y _i = β _0 + βx _i + \epsilon _i$

$Y \sim N(0,\sigma^2)$,

и дисперсии $\sigma^2 _1 = \sigma^2 _2 = ... = \sigma^2 _i$ для каждого $Y _i$ 

>- К счастью, поскольку $\epsilon _i \sim N(0,\sigma^2)$, можно проверить равенство дисперсий остатков $\epsilon _i$

<br />
>- Нужно и важно для тестов параметров

>- Проверяем на графике остатков по отношению к предсказанным значениям

>- Можно сделать тест С Кокрана (Cochran's C), но только если несколько значений y для каждого x

*** right

![Условие нормальности и гомогенность дисперсий](./assets/img/normality-assumption.png "Условие нормальности и гомогенность дисперсий")

<div class="footnote">Рисунок из кн. Watkins et al., 2008, стр. 743, рис. 11.4</div>

--- &twocol

Диагностика регрессии по графикам остатков
========================================================

*** left

![Диагностика регрессии по графикам остатков](./assets/img/assumption-violations-on-residual-plots.png "Диагностика регрессии по графикам остатков")

<div class="footnote">Рисунок из кн. Logan, 2010, стр. 174, рис. 8.5 d</div>

*** right

<br />
>- условия:
  - a - все выполнены
  - b - разброс остатков разный (wedge-shaped pattern)
  - c - разброс остатков одинаковый, но нужны дополнительные предикторы
  - d - к нелинейной зависимости применили линейную регрессию

--- .prompt &twocol

Скажите, 
========================================================

- какой регрессии соответствует какой график остатков?
- все ли условия применимости регрессии здесь выполняются?
- назовите случаи, в которых можно и нельзя применить линейную регрессию?

*** left

![Условия применимости-квиз](./assets/img/assumption-quiz1.png "Условия применимости-квиз")

*** right

![Условия применимости-квиз](./assets/img/assumption-quiz2.png "Условия применимости-квиз")

<div class="footnote">Рисунок из кн. Watkins et al. 2008, стр. 177, рис. 3.84-3.85</div>

*** pnotes

- A1 - нелинейная связь - нельзя; 
- B2 - все в порядке, можно; 
- C3 - все в порядке, можно; 
- D4 - синусоидальный тренд в остатках, нарушено условие независимости или зависимость нелинейная - нельзя.

--- &twocol

Какие наблюдения влияют на ход регрессии больше других?
========================================================

*** left

Влиятельные наблюдения, выбросы, outliers

- большая абсолютная величина остатка
- близость к краям области определения ([leverage](http://en.wikipedia.org/wiki/Leverage_(statistics\) ) - рычаг, "сила"; иногда называют hat)

<br />
>-
  - 1 - не влияет
  - 2 - умеренно влияет (большой остаток, малая сила влияния)
  - 3 - очень сильно влияет (большой остаток, большая сила влияния)

*** right

![Влиятельные наблюдения](./assets/img/influential-observations.png "Влиятельные наблюдения")

<div class="footnote">Рисунок из кн. Quinn, Keough, 2002, стр. 96, рис. 5.8</div>

--- &twocol

Как оценить влиятельность наблюдений
========================================================

*** left

[Расстояние Кука (Cook's d)](http://en.wikipedia.org/wiki/Cook's_distance) (Cook, 1977)

>- Учитывает одновременно величину остатка и близость к краям области определения (leverage)

>- Условное пороговое значение:  
выброс, если $d \ge 4/(N - k - 1)$,  
где $N$ - объем выборки,  
$k$ - число предикторов.  

>- Дж. Фокс советует не обращать внимания на пороговые значения (Fox, 1991).

>- Что делать с влиятельными точками?
  - Проверить, не ошибка ли это.  
    Если это не ошибка, не удалять - обсуждать!
  - Проверить, что будет, если их исключить из модели

*** right

![Влиятельные наблюдения](./assets/img/influential-observations.png "Влиятельные наблюдения")

<div class="footnote">Рисунок из кн. Quinn, Keough, 2002, стр. 96, рис. 5.8</div>

--- &twocol

Что делать с выбросами?
========================================================

*** left

  - Проверить, не ошибка ли это.  
    Если это не ошибка, не удалять - обсуждать!
  - Проверить, что будет, если их исключить из модели

*** right

![Влиятельные наблюдения](./assets/img/influential-observations.png "Влиятельные наблюдения")

<div class="footnote">Рисунок из кн. Quinn, Keough, 2002, стр. 96, рис. 5.8</div>


--- .segue

Проверим условия применимости
========================================================


--- .prompt

Проверьте линейность связи, 
========================================================
постройте для этого график рассеяния


```r
ggplot()
aes()
geom_point()
```



*** pnotes


```r
ggplot(data = nelson, aes(x = humidity, y = weightloss)) + 
  geom_point()
```

<img src="figure/unnamed-chunk-8.png" title="plot of chunk unnamed-chunk-8" alt="plot of chunk unnamed-chunk-8" style="display: block; margin: auto;" />


---

Для анализа остатков выделим нужные данные в новый датафрейм
========================================================


```r
# нам нужна линейная регрессия из прошлой лекции
nelson_lm <- lm(weightloss ~ humidity, nelson) # линейная регрессия
# library(ggplot2) # функция fortify() находится в пакете ggplot2
nelson_diag <- fortify(nelson_lm)
names(nelson_diag) # названия переменных
```

```
## [1] "weightloss" "humidity"   ".hat"       ".sigma"     ".cooksd"   
## [6] ".fitted"    ".resid"     ".stdresid"
```


<br />

>- Кроме `weightloss` и `humidity` нам понадобятся  
  - `.cooksd` - расстояние Кука  
  - `.fitted` - предсказанные значения  
  - `.resid` - остатки  
  - `.stdresid` - стандартизованные остатки

--- .prompt

Постройте график зависимости остатков от предиктора,  
========================================================
используя данные из `nelson_diag`

- `humidity` - относительная влажность (наш предиктор)
- `.resid` - остатки  


```r
names() 
ggplot()
aes()
geom_point()
```


>- По абсолютным остаткам сложно сказать, большие они или маленькие.  Нужна стандартизация


*** pnotes


```r
ggplot(data = nelson_diag, aes(x = humidity, y = .resid)) +
  geom_point()
```

<img src="figure/unnamed-chunk-11.png" title="plot of chunk unnamed-chunk-11" alt="plot of chunk unnamed-chunk-11" style="display: block; margin: auto;" />

По абсолютным остаткам сложно сказать, большие они или маленькие. Нужна стандартизация

--- .prompt

Постройте график зависимости __стандартизованных остатков__ от предсказанных значений
========================================================

Стандартизованные остатки $\frac {y _i - \hat y _i} {\sqrt{MS _e}}$
- можно сравнивать между регрессиями
- можно сказать, какие остатки большие, какие нет
  - $\le 2 SD$ - обычные
  - $> 3 SD$ - редкие

Использйте данные из `nelson_diag`  
- `.fitted` - предсказанные значения  
- `.resid` - остатки  


```r
ggplot()
aes()
geom_point()
```



*** pnotes


```r
ggplot(data = nelson_diag, aes(x = .fitted, y = .stdresid)) +
  geom_point()
```

<img src="figure/unnamed-chunk-13.png" title="plot of chunk unnamed-chunk-13" alt="plot of chunk unnamed-chunk-13" style="display: block; margin: auto;" />


---

График станет информативнее, если кое-что добавить
========================================================


```r
ggplot(data = nelson_diag, aes(x = .fitted, y = .stdresid)) +
  geom_point(aes(size = .cooksd)) +          # расстояние Кука
  geom_smooth(method="loess", se = FALSE) +  # линия тренда, сглаживание локальной регрессией
  geom_hline(yintercept = 0)                 # горизонтальная линия на уровне y = 0
```

<img src="figure/unnamed-chunk-14.png" title="plot of chunk unnamed-chunk-14" alt="plot of chunk unnamed-chunk-14" style="display: block; margin: auto;" />


--- &twocol

Какие выводы можно сделать по графику остатков?
========================================================

*** left

<img src="figure/resplot.png" title="plot of chunk resplot" alt="plot of chunk resplot" style="display: block; margin: auto;" />


*** right

>- Стандартизованные остатки умеренной величины (в пределах двух стандартных отклонений), их разброс почти одинаков
>- Мало точек, чтобы надежно оценить наличие трендов среди остатков

--- &twocol

Нормально-вероятностный график стандартизованных остатков
========================================================


```r
mean_val <- mean(nelson_diag$.stdresid)  
sd_val <- sd(nelson_diag$.stdresid)
quantile_plot <- ggplot(nelson_diag, aes(sample = .stdresid)) + 
  geom_point(stat = "qq") +
geom_abline(intercept = mean_val, slope = sd_val) + # на эту линию должны ложиться значения
  labs(x = "Квантили стандартного нормального распределения", y = "Квантили набора данных")
quantile_plot
```

<br />

*** left

<img src="figure/unnamed-chunk-15.png" title="plot of chunk unnamed-chunk-15" alt="plot of chunk unnamed-chunk-15" style="display: block; margin: auto;" />


*** right

Используется, чтобы оценить форму распределения.

Если точки лежат на одной прямой - нормальное распределение.

<br />
>- Небольшие отклонения от нормального распределения, но мало точек, чтобы оценить с уверенностью

--- .segue

Мощность линейной регрессии 
========================================================

---

Величина эффекта из общих соображений
========================================================

```r
library(pwr)
cohen.ES(test="f2",size="large")
```

```
## 
##      Conventional effect size from Cohen (1982) 
## 
##            test = f2
##            size = large
##     effect.size = 0.35
```


*** pnotes

Слабый | Умеренный | Сильный
--- | --- | ---
0.02 | 0.15 | 0.35

---

Величину эффекта можно оценить по $R^2$
======================================================

$$f^2 = \frac {R^2}{1 - R^2}$$

$R^2$ - коэффициент детерминации

--- .prompt

Посчитайте 
======================================================
какой нужен объем выборки, чтобы с вероятностью 0.8 обнаружить зависимость при помощи простой линейной регрессии, если ожидается $R^2 = 0.6$ ?

$$f^2 = \frac {R^2}{1 - R^2}$$


```r
pwr.f2.test()
```


*** pnotes


```r
effect <- 0.6 * (1 - 0.6)
pwr.f2.test(u=1, v=NULL, f2=effect, sig.level=0.05, power=0.8)
```

```
## 
##      Multiple regression power calculation 
## 
##               u = 1
##               v = 32.7
##              f2 = 0.24
##       sig.level = 0.05
##           power = 0.8
```


---

Take home messages
========================================================
>- Условия применимости простой линейной регрессии должны выполняться, чтобы тестировать гипотезы
  1. Независимость
  1. Линейность 
  1. Нормальное распределение
  1. Гомогенность дисперсий
  
>- Мощность линейной регрессии можно рассчитать как мощность F-критерия. Величину эффекта можно оценить по $R^2$

---

Дополнительные ресурсы
========================================================

- Logan, 2010, pp. 170-207
- Quinn, Keough, 2002, pp. 92-104
- [Open Intro to Statistics](http://www.openintro.org/stat/), pp. 315-353.

